defaults:
  - _self_
  - model: efficientnet #モデル（simplecnn / resnet/ efficientnet/ mobilenet）
  - optimizer: adam  #Optimizer（adam / sgd/ adamw）
  - lr_scheduler: cosine #スケジューラー（plateau or multi_step or cosine）

args:
  history_dir: "./outputs/train_reg/history"
  train_id: "your_train_id_here"  # 実行時に自動上書き
  seed: 50
  skip_tested: false


seed: 50 #再現性確保のため、固定値を採用
epoch: 300

# ステップ2で "latest" を指定して上書きするための項目
load_weights_from: null

amp: true #AMP

# データローダ設定
dataloader: 
  batch_size: 64 # バッチサイズ
  num_workers: 8  # ← 16 から減らす 同様の理由
  pin_memory: true

# 勾配累積：実質的なバッチサイズを 32 * 2 = 64 にする(※無しの場合：デフォルトの１となる)
accumulation_steps: 1

#---データセットの指定---
dataset:
  name: HDR+burst
  split_root: conf/dataset/${dataset.name}_split
  # image_root:「実際の画像ファイルが保存されている基点ディレクトリ」
  #image_root: conf/dataset/${dataset.name}/20171106/results_20171023
  image_root: conf/dataset/${dataset.name}/processed_1024px_linear_exr

  normalize_mean: [0.1191, 0.0917, 0.0823] #mean:[0.3925, 0.3757, 0.3692],std:[0.2424, 0.2433, 0.2818]変更前
  normalize_std: [0.1649, 0.1313, 0.1368]  #値が0～1の範囲に収まらなければならない

  #mean:[0.3925, 0.3757, 0.3692] (フルサイズのデータセットで行った仮の結果)　10／20
  #std:[0.2424, 0.2433, 0.2818]
  train:
      csv_file: ${dataset.split_root}/train.csv
      root: ${dataset.image_root}
      transform:
        random_resized_crop: # 画像をランダムな位置・サイズで切り抜き、224x224にリサイズする
          size: 224
          scale: [0.8, 1.0] # 元画像を指定の範囲で切り抜く
          ratio: [0.75, 1.33]
        random_horizontal_flip: # 画像を左右反転
          p: 0.5
        random_vertical_flip: # 画像を上下反転
          p: 0.5
        random_rotation: # -15度から+15度の範囲で画像をランダムに回転
          degrees: 10
        #color_jitter:  # 明るさ、コントラスト、彩度、色相をランダムに変化させる
          #brightness: 0.2
          #contrast: 0.2
          #saturation: 0.2
          #hue: 0.1
          # 強力な正則化手法「RandomErasing」を追加
        random_erasing:
          p: 0.2           # 25%の確率で実行
          scale: [0.02, 0.1]
          ratio: [0.3, 3.3]
          # 正規化:学習を安定させるための必須の前処理
        normalize:
          mean: ${dataset.normalize_mean} 
          std: ${dataset.normalize_std} 

  #検証用データ
  val:
    csv_file: ${dataset.split_root}/val.csv
    root: ${dataset.image_root}
    transform:
      resize: 256
      center_crop: 224
      normalize:
        mean: ${dataset.normalize_mean} # ◀ 共通の値を参照
        std: ${dataset.normalize_std} 

  # --- 評価（試験）用のデータセット設定 ---
  test:
    csv_file: ${dataset.split_root}/test.csv
    root: ${dataset.image_root}
    
    # 評価時にはデータ拡張を行わない
    transform:
      resize: 256
      center_crop: 224
      normalize:
        mean: ${dataset.normalize_mean} # ◀ 共通の値を参照
        std: ${dataset.normalize_std} 