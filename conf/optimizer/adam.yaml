# torch.optim.Adam をインスタンス化するよう指定。
# Python側で `hydra.utils.instantiate(cfg.optimizer, params=model.parameters())` 
# のように呼び出すことで、params以外の引数(lr, weight_decay)が渡される。
name: adam

params:
  lr: 0.0003 #学習率を調整
  betas:
    - 0.9
    - 0.999
  eps: 1e-08
  weight_decay: 0.0002
  amsgrad: false